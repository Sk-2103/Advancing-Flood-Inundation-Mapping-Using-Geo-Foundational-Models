{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55051e3c-b4ac-43bc-b637-508c847a8189",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b45ed5-3108-4f59-a625-173158ef0ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import h5py\n",
    "import rasterio\n",
    "import albumentations\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from albumentations import Compose, Resize, HorizontalFlip, Flip\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import terratorch\n",
    "from terratorch.datamodules import Landslide4SenseNonGeoDataModule\n",
    "from terratorch.datasets import Landslide4SenseNonGeo\n",
    "from terratorch.tasks import SemanticSegmentationTask\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download config.json to the specified folder\n",
    "hf_hub_download(\n",
    "    repo_id='ibm-nasa-geospatial/Prithvi-EO-2.0-300M',\n",
    "    filename=\"config.json\",\n",
    "    cache_dir='/home/skaushik/flood_prithvi/'\n",
    ")\n",
    "\n",
    "\n",
    "# Custom FloodDataModule\n",
    "class FloodDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_root, batch_size=2, train_transform=None, val_transform=None, test_transform=None):\n",
    "        super().__init__()\n",
    "        self.data_root = data_root\n",
    "        self.train_dir = os.path.join(data_root, \"images/train\")\n",
    "        self.val_dir = os.path.join(data_root, \"images/validation\")\n",
    "        self.test_dir = os.path.join(data_root, \"images/test\")\n",
    "        self.mask_train_dir = os.path.join(data_root, \"annotations/train\")\n",
    "        self.mask_val_dir = os.path.join(data_root, \"annotations/validation\")\n",
    "        self.mask_test_dir = os.path.join(data_root, \"annotations/test\")\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_transform = train_transform\n",
    "        self.val_transform = val_transform\n",
    "        self.test_transform = test_transform\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage in (\"fit\", None):\n",
    "            self.train_dataset = self.create_dataset(self.train_dir, self.mask_train_dir, self.train_transform)\n",
    "            self.val_dataset = self.create_dataset(self.val_dir, self.mask_val_dir, self.val_transform)\n",
    "\n",
    "        if stage in (\"test\", None):\n",
    "            self.test_dataset = self.create_dataset(self.test_dir, self.mask_test_dir, self.test_transform)\n",
    "\n",
    "    def create_dataset(self, image_dir, mask_dir, transform):\n",
    "        return CustomFloodDataset(image_dir, mask_dir, transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomFloodDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "\n",
    "        # Read the image and mask using rasterio\n",
    "        with rasterio.open(image_path) as src:\n",
    "            image = src.read()  # (bands, height, width)\n",
    "        with rasterio.open(mask_path) as src:\n",
    "            mask = src.read(1)  # Single channel for mask\n",
    "\n",
    "        mask = (mask > 0.5).astype(np.uint8)  # Binarize mask\n",
    "        image = np.moveaxis(image, 0, -1)  # Convert to (height, width, bands)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Include the filename in the output\n",
    "        filename = os.path.basename(image_path)\n",
    "\n",
    "        return {\"image\": image, \"mask\": mask, \"filename\": filename}\n",
    "\n",
    "#    def plot(self, sample):\n",
    "#        \"\"\"Plot a sample from the dataset.\"\"\"\n",
    "#        image = sample[\"image\"].permute(1, 2, 0).numpy()  # Convert to HWC\n",
    "#        mask = sample[\"mask\"].numpy()\n",
    "#\n",
    "#        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#        ax[0].imshow(image)\n",
    "#        ax[0].set_title(\"Image\")\n",
    "#        ax[0].axis(\"off\")\n",
    "#\n",
    "#        ax[1].imshow(mask, cmap=\"gray\")\n",
    "#        ax[1].set_title(\"Mask\")\n",
    "#        ax[1].axis(\"off\")\n",
    "#\n",
    "#        plt.tight_layout()\n",
    "#        plt.show()\n",
    "#\n",
    "#        return fig\n",
    "\n",
    "# Transforms\n",
    "train_transform = Compose([\n",
    "    HorizontalFlip(),\n",
    "    Resize(896, 896),  # Ensure size is divisible by 14 and closer to 1024\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize(896, 896),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Resize(896, 896),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Instantiate FloodDataModule\n",
    "data_module = FloodDataModule(\n",
    "    data_root='/home/skaushik/flood_prithvi/cambodia',\n",
    "    batch_size=2,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=test_transform,\n",
    ")\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"flood_logs\",\n",
    "    name=\"flood_segmentation\"\n",
    ")\n",
    "\n",
    "# Define ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/\",  # Save all best models in a single directory\n",
    "    filename=\"epoch-{epoch:02d}-val_f1-{val/Multiclass_F1_Score:.4f}\",  # Include epoch and F1 in filename\n",
    "    monitor=\"val/Multiclass_F1_Score\",  # Track validation F1-score\n",
    "    mode=\"max\",  # Save models with highest F1-score\n",
    "    save_top_k=2,  # Keep only the best 3 models\n",
    "    every_n_epochs=1,\n",
    "    save_on_train_epoch_end=False,  # Save based on validation performance\n",
    "    auto_insert_metric_name=False  # Prevents creating subfolders for each epoch\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = SemanticSegmentationTask(\n",
    "    model_args={\n",
    "        \"decoder\": \"UperNetDecoder\",\n",
    "        \"backbone_pretrained\": True,\n",
    "        \"backbone\": \"prithvi_eo_v2_600\",\n",
    "        \"backbone_in_channels\": 4,  # Match your dataset's number of channels\n",
    "        \"rescale\": True,\n",
    "        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_BROAD\"],\n",
    "        \"backbone_num_frames\": 1,\n",
    "        \"num_classes\": 2,\n",
    "        \"head_dropout\": 0.1,\n",
    "        \"decoder_channels\": 256,\n",
    "        \"decoder_scale_modules\": True,\n",
    "        \"head_channel_list\": [128, 64],\n",
    "        \"necks\": [\n",
    "            {\n",
    "                \"name\": \"SelectIndices\",\n",
    "                \"indices\": [7, 15, 23, 31]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ReshapeTokensToImage\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    plot_on_val=False,  # Enable plotting during validation (set False to skip)\n",
    "    loss=\"focal\",\n",
    "    lr=1e-4,\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_hparams={\"weight_decay\": 0.1},\n",
    "    scheduler=\"StepLR\",\n",
    "    scheduler_hparams={\"step_size\": 10, \"gamma\": 0.9},\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    ")\n",
    "\n",
    "# Train\n",
    "#trainer.fit(model, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c57aa3-282b-43ba-9f61-75abb7ae2eac",
   "metadata": {},
   "source": [
    "### Testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27945f6e-f5a4-4a64-b111-57f70767fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.test(model, datamodule=data_module, ckpt_path=\"/home/skaushik/Prithvi/Prithvi/Prithvi-EO-2.0-main/checkpoints/checkpoints/cambodia/epoch-27-val_f1-0.8856.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1f9d7-687b-4aea-98c1-619de1b98dce",
   "metadata": {},
   "source": [
    "## inference and saving predicitons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e91a70-4213-413e-b329-de50c6ced6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class CustomFloodDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "\n",
    "        # Read the image and mask using rasterio\n",
    "        with rasterio.open(image_path) as src:\n",
    "            image = src.read()  # (bands, height, width)\n",
    "        with rasterio.open(mask_path) as src:\n",
    "            mask = src.read(1)  # Single channel for mask\n",
    "\n",
    "        mask = (mask > 0.5).astype(np.uint8)  # Binarize mask\n",
    "        image = np.moveaxis(image, 0, -1)  # Convert to (height, width, bands)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        filename = os.path.basename(image_path)\n",
    "        return {\"image\": image, \"mask\": mask, \"filename\": filename}\n",
    "\n",
    "# Data Module\n",
    "class FloodDataModule:\n",
    "    def __init__(self, data_root, batch_size=2, test_transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.test_dir = os.path.join(data_root, \"images/test\")\n",
    "        self.mask_test_dir = os.path.join(data_root, \"annotations/test\")\n",
    "        self.batch_size = batch_size\n",
    "        self.test_transform = test_transform\n",
    "\n",
    "    def setup(self):\n",
    "        self.test_dataset = CustomFloodDataset(self.test_dir, self.mask_test_dir, self.test_transform)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "# Function to plot and save the image, ground truth mask, and predicted mask\n",
    "\n",
    "def plot_and_save_sample(image_path, mask, prediction, filename, output_dir):\n",
    "    \"\"\"\n",
    "    Plots the input image (using bands 4, 3, 2), ground truth mask, and predicted mask side by side,\n",
    "    and saves the predicted mask to the specified output directory.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): The file path to the input image.\n",
    "    - mask (torch.Tensor): The ground truth mask tensor of shape (H, W).\n",
    "    - prediction (np.ndarray): The predicted mask of shape (H, W).\n",
    "    - filename (str): The filename of the image being processed.\n",
    "    - output_dir (str): The directory where the predicted mask will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Read the specific bands (4, 3, 2) using rasterio\n",
    "    with rasterio.open(image_path) as src:\n",
    "        # Read bands 4, 3, 2\n",
    "        band4 = src.read(4)\n",
    "        band3 = src.read(3)\n",
    "        band2 = src.read(2)\n",
    "\n",
    "        # Stack bands to create an RGB image\n",
    "        image_rgb = np.stack((band4, band3, band2), axis=-1)\n",
    "\n",
    "        # Normalize the image for display\n",
    "        image_rgb = image_rgb.astype(np.float32)\n",
    "        image_rgb /= image_rgb.max()\n",
    "\n",
    "    # Plot the image, ground truth mask, and predicted mask\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle(f\"Sample: {filename}\", fontsize=16)\n",
    "\n",
    "    # Display the RGB image\n",
    "    ax[0].imshow(image_rgb)\n",
    "    ax[0].set_title(\"Image (Bands 4, 3, 2)\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    # Display the ground truth mask\n",
    "    ax[1].imshow(mask.cpu().numpy(), cmap=\"gray\")\n",
    "    ax[1].set_title(\"Ground Truth Mask\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    # Display the predicted mask\n",
    "    ax[2].imshow(prediction, cmap=\"gray\")\n",
    "    ax[2].set_title(\"Predicted Mask\")\n",
    "    ax[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the predicted mask\n",
    "    # Normalize the prediction to the range [0, 255]\n",
    "    prediction_uint8 = (prediction * 255).astype(np.uint8)\n",
    "    # Create a PIL image from the NumPy array\n",
    "    prediction_img = Image.fromarray(prediction_uint8)\n",
    "    # Define the output path\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    # Save the image\n",
    "    prediction_img.save(output_path)\n",
    "    print(f\"Saved prediction to {output_path}\")\n",
    "\n",
    "# Ensure the Data Module is set up\n",
    "data_module.setup()\n",
    "test_loader = data_module.test_dataloader()  # Create test_loader\n",
    "\n",
    "# Define the output directory for predictions\n",
    "output_dir = '/home/skaushik/flood_prithvi/predictions_smoalia_ps'\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Run inference and plot/save results\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        masks = batch[\"mask\"].to(device)\n",
    "        filenames = batch[\"filename\"]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs.output, dim=1).cpu().numpy()\n",
    "\n",
    "        # Plot and save each sample in the batch\n",
    "        for i in range(images.size(0)):\n",
    "            image_path = os.path.join(data_module.test_dir, filenames[i])\n",
    "            plot_and_save_sample(image_path, masks[i], preds[i], filenames[i], output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
